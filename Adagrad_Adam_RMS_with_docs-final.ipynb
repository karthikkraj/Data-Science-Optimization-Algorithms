{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7ac7e34e",
      "metadata": {
        "id": "7ac7e34e"
      },
      "source": [
        "**Group Details:**\n",
        "\n",
        "- 2022BCD0038 - K. Sri Chaitan\n",
        "- 2022BCD0041 - Karthik Raj\n",
        "- 2022BCD0021 - Siddharth Chitikesi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5209f57",
      "metadata": {
        "id": "b5209f57"
      },
      "source": [
        "\n",
        "# Optimization Paths for the Rosenbrock Function Using Adagrad, Adam, and RMSprop\n",
        "\n",
        "This notebook demonstrates the optimization paths taken by three different optimization methods—**Adagrad**, **Adam**, and **RMSprop**—to minimize the Rosenbrock function. These gradient-based optimization algorithms are commonly used in machine learning for parameter optimization, and each employs unique strategies to adjust the learning rate during optimization.\n",
        "\n",
        "The Rosenbrock function, given by:\n",
        "\n",
        "$$ f(x, y) = (1 - x)^2 + 100(y - x^2)^2 $$\n",
        "\n",
        "is a well-known test problem for optimization methods, as it has a narrow, curved valley making it challenging to optimize. Each of these optimization algorithms leverages different approaches to effectively navigate such complex landscapes.\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce16d3e5",
      "metadata": {
        "id": "ce16d3e5"
      },
      "source": [
        "\n",
        "# Import Libraries\n",
        "\n",
        "This cell imports the necessary libraries for implementing and visualizing the optimization algorithms.\n",
        "\n",
        "- `numpy` is used for numerical calculations, particularly for handling matrix operations and vectorized computations.\n",
        "- `matplotlib.pyplot` is used for plotting the optimization paths.\n",
        "- Other specialized libraries or functions may be imported within each algorithm section if needed.\n",
        "\n",
        "These libraries are essential for performing mathematical operations, handling array data structures, and visualizing the results of each optimization method on the Rosenbrock function.\n",
        "    "
      ]
    },
    {
      "cell_type": "raw",
      "id": "15096db7-53dc-4eb5-98bf-96c300e37ffa",
      "metadata": {
        "id": "15096db7-53dc-4eb5-98bf-96c300e37ffa"
      },
      "source": [
        "Group Members:\n",
        "- Ch. Siddharth (BCD21)\n",
        "- K Sri Chaitan (BCD38)\n",
        "- K Karthik Raj (BCD41)"
      ]
    },
    {
      "cell_type": "raw",
      "id": "5efd8c00-0814-494e-a2f4-79c0e32bb5aa",
      "metadata": {
        "id": "5efd8c00-0814-494e-a2f4-79c0e32bb5aa"
      },
      "source": [
        "Documentation Summary:\n",
        "\n",
        "rosenbrock_function: Computes the value of the Rosenbrock function at a given point,\n",
        "which is often used for testing optimization algorithms due to its curved valley.\n",
        "\n",
        "rosenbrock_gradient: Computes the gradient (vector of partial derivatives) of the Rosenbrock function,\n",
        "which helps determine the direction of steepest ascent or descent.\n",
        "\n",
        "Adagrad, RMSProp, and Adam: These are popular optimization algorithms.\n",
        "\n",
        "Each one uses a different technique to adjust the learning rate based on the history of gradients.\n",
        "In this code, we run each algorithm for 500 iterations, storing the path of the optimization process."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries\n",
        "\n",
        "This cell imports the necessary libraries for implementing and visualizing the optimization algorithms.\n",
        "\n",
        "- `numpy` is used for numerical calculations, particularly for handling matrix operations and vectorized computations.\n",
        "- `matplotlib.pyplot` is used for plotting the optimization paths.\n"
      ],
      "metadata": {
        "id": "uRHSqDdqc074"
      },
      "id": "uRHSqDdqc074"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a03eeeef-6dac-4623-9315-49605b4aea5e",
      "metadata": {
        "id": "a03eeeef-6dac-4623-9315-49605b4aea5e"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rosenbrock Function and Gradient\n",
        "\n",
        "This cell defines the **Rosenbrock function** and its gradient. The Rosenbrock function is a popular test problem for optimization, defined as:\n",
        "\n",
        "$$ f(x, y) = (1 - x)^2 + 100(y - x^2)^2 $$\n",
        "\n",
        "The gradients `df_dx` and `df_dy` are necessary for gradient-based optimization methods like Adagrad, Adam, and RMSprop.\n"
      ],
      "metadata": {
        "id": "IEiMbNpbc355"
      },
      "id": "IEiMbNpbc355"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47996d7-6bf7-480e-876e-c31a2a057997",
      "metadata": {
        "id": "f47996d7-6bf7-480e-876e-c31a2a057997"
      },
      "outputs": [],
      "source": [
        "# Define the Rosenbrock function\n",
        "def rosenbrock(w):\n",
        "    x, y = w\n",
        "    return (1 - x)**2 + 100 * (y - x**2)**2\n",
        "\n",
        "# Define the gradient of the Rosenbrock function\n",
        "def rosenbrock_grad(w):\n",
        "    x, y = w\n",
        "    grad_x = -2 * (1 - x) - 400 * x * (y - x**2)\n",
        "    grad_y = 200 * (y - x**2)\n",
        "    return np.array([grad_x, grad_y])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adagrad Implementation\n",
        "\n",
        "This cell implements the **Adagrad** optimization algorithm. Adagrad adjusts the learning rate for each parameter based on the cumulative sum of squared gradients, reducing the step size for frequently updated parameters.\n",
        "\n",
        "In this function:\n",
        "- `grad_square_sum` keeps track of the accumulated squared gradients.\n",
        "- `adjusted_grad` adjusts the learning rate to adapt to the parameter scale.\n",
        "- The method iteratively updates parameters `x` and `y` until convergence or the maximum number of iterations is reached.\n"
      ],
      "metadata": {
        "id": "-diQapJHc7RA"
      },
      "id": "-diQapJHc7RA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1cd6b17-2641-430e-b8eb-490c7719d3d3",
      "metadata": {
        "id": "f1cd6b17-2641-430e-b8eb-490c7719d3d3"
      },
      "outputs": [],
      "source": [
        "# Adagrad optimizer with path tracking\n",
        "def adagrad_path(w_init, grad_fn, alpha=0.01, epsilon=1e-6, max_iters=10000):\n",
        "    w = np.array(w_init, dtype=np.float64)\n",
        "    grad_accum = np.zeros_like(w)\n",
        "    path = [w.copy()]\n",
        "    for k in range(max_iters):\n",
        "        grad = grad_fn(w)\n",
        "        if np.linalg.norm(grad) < epsilon:\n",
        "            break\n",
        "        grad_accum += grad**2\n",
        "        w -= alpha * grad / (np.sqrt(grad_accum) + epsilon)\n",
        "        path.append(w.copy())\n",
        "    return np.array(path), k"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSprop Implementation\n",
        "\n",
        "This cell contains the **RMSprop** optimization algorithm, which adjusts the learning rate based on the moving average of squared gradients.\n",
        "\n",
        "RMSprop parameters include:\n",
        "- `beta`, controlling the decay rate for the moving average.\n",
        "- `grad_square_avg`, which maintains the running average of squared gradients.\n",
        "\n",
        "RMSprop is well-suited for non-stationary objectives and addresses the issues of Adagrad’s rapidly decaying learning rate.\n"
      ],
      "metadata": {
        "id": "QcVPpZ3jc_TH"
      },
      "id": "QcVPpZ3jc_TH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cbd3f0d-71bb-4a6f-bb46-b07abaf1e090",
      "metadata": {
        "id": "4cbd3f0d-71bb-4a6f-bb46-b07abaf1e090"
      },
      "outputs": [],
      "source": [
        "# RMSProp optimizer with path tracking\n",
        "def rmsprop_path(w_init, grad_fn, alpha=0.001, beta=0.9, epsilon=1e-6, max_iters=10000):\n",
        "    w = np.array(w_init, dtype=np.float64)\n",
        "    grad_accum = np.zeros_like(w)\n",
        "    path = [w.copy()]\n",
        "    for k in range(max_iters):\n",
        "        grad = grad_fn(w)\n",
        "        if np.linalg.norm(grad) < epsilon:\n",
        "            break\n",
        "        grad_accum = beta * grad_accum + (1 - beta) * grad**2\n",
        "        w -= alpha * grad / (np.sqrt(grad_accum) + epsilon)\n",
        "        path.append(w.copy())\n",
        "    return np.array(path), k"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adam Implementation\n",
        "\n",
        "This cell implements the **Adam** optimization algorithm, which combines elements of both Adagrad and RMSprop. Adam uses running averages of both gradients and squared gradients to adapt the learning rate.\n",
        "\n",
        "Key parameters in Adam include:\n",
        "- `beta1` and `beta2`, which control the decay rates for the moving averages of the gradient (`m`) and squared gradient (`v`).\n",
        "- `m_hat` and `v_hat`, bias-corrected versions of `m` and `v`, prevent initial bias toward zero.\n",
        "\n",
        "Adam is generally faster to converge and is widely used in machine learning applications.\n"
      ],
      "metadata": {
        "id": "XORO5uUKdMP0"
      },
      "id": "XORO5uUKdMP0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121f44c2-933f-43d2-b1b4-265f3b0ea63f",
      "metadata": {
        "id": "121f44c2-933f-43d2-b1b4-265f3b0ea63f"
      },
      "outputs": [],
      "source": [
        "# Adam optimizer with path tracking\n",
        "def adam_path(w_init, grad_fn, alpha=0.01, beta1=0.9, beta2=0.999, epsilon=1e-6, max_iters=10000):\n",
        "    w = np.array(w_init, dtype=np.float64)\n",
        "    m, v = np.zeros_like(w), np.zeros_like(w)\n",
        "    path = [w.copy()]\n",
        "    for k in range(1, max_iters + 1):\n",
        "        grad = grad_fn(w)\n",
        "        if np.linalg.norm(grad) < epsilon:\n",
        "            break\n",
        "        m = beta1 * m + (1 - beta1) * grad\n",
        "        v = beta2 * v + (1 - beta2) * grad**2\n",
        "        m_hat, v_hat = m / (1 - beta1**k), v / (1 - beta2**k)\n",
        "        w -= alpha * m_hat / (np.sqrt(v_hat) + epsilon)\n",
        "        path.append(w.copy())\n",
        "    return np.array(path), k"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plotting Optimization Paths\n",
        "\n",
        "This cell is responsible for visualizing the optimization paths taken by the Adagrad, Adam, and RMSprop algorithms on the Rosenbrock function. The contour plot shows the landscape of the Rosenbrock function, while the paths demonstrate each optimizer’s convergence behavior.\n",
        "\n",
        "The function `plot_paths`:\n",
        "- Plots the contour map of the Rosenbrock function.\n",
        "- Overlays the paths of each optimization algorithm to allow for visual comparison.\n"
      ],
      "metadata": {
        "id": "ObvSGbQOdPmY"
      },
      "id": "ObvSGbQOdPmY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3777ef7a-e880-4575-87ef-2f7feacd5b4f",
      "metadata": {
        "id": "3777ef7a-e880-4575-87ef-2f7feacd5b4f"
      },
      "outputs": [],
      "source": [
        "# Initial point\n",
        "w_init = [0.0, 1.5]\n",
        "\n",
        "# Generate paths and iteration counts\n",
        "adagrad_path_points, adagrad_iters = adagrad_path(w_init, rosenbrock_grad, alpha=0.01, epsilon=1e-6)\n",
        "rmsprop_path_points, rmsprop_iters = rmsprop_path(w_init, rosenbrock_grad, alpha=0.001, epsilon=1e-6)\n",
        "adam_path_points, adam_iters = adam_path(w_init, rosenbrock_grad, alpha=0.01, epsilon=1e-6)\n",
        "\n",
        "# Contour plot of the Rosenbrock function\n",
        "x = np.linspace(-2, 2, 400)\n",
        "y = np.linspace(-1, 3, 400)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "Z = rosenbrock([X, Y])\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Adjust contour levels and use a more visible color map\n",
        "levels = np.logspace(0.1, 3.5, 30)  # Adjusted levels for better contour resolution\n",
        "contour = plt.contour(X, Y, Z, levels=levels, norm=LogNorm(), cmap='plasma')  # Using 'plasma' for better contrast\n",
        "plt.clabel(contour, inline=1, fontsize=10)\n",
        "\n",
        "# Plot paths with distinct markers\n",
        "plt.plot(adagrad_path_points[:, 0], adagrad_path_points[:, 1], 'o-', color=\"cyan\", label=\"Adagrad\", markersize=5)\n",
        "plt.plot(rmsprop_path_points[:, 0], rmsprop_path_points[:, 1], 's-', color=\"magenta\", label=\"RMSProp\", markersize=5)\n",
        "plt.plot(adam_path_points[:, 0], adam_path_points[:, 1], 'x-', color=\"red\", label=\"Adam\", markersize=5)\n",
        "\n",
        "# Starting and optimal points\n",
        "plt.plot(w_init[0], w_init[1], 'ko', markersize=8, label=\"Start\")\n",
        "plt.plot(1, 1, 'k*', markersize=15, label=\"Optimal (1, 1)\")\n",
        "\n",
        "# Color bar and labels\n",
        "plt.colorbar(contour, label='Rosenbrock Function Value')\n",
        "plt.title(f\"Optimizer Paths on Rosenbrock Contour\\n\"\n",
        "          f\"Adagrad: {adagrad_iters} iters, RMSProp: {rmsprop_iters} iters, Adam: {adam_iters} iters\")\n",
        "plt.xlabel(\"w1\")\n",
        "plt.ylabel(\"w2\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "raw",
      "id": "4050c112-3ff1-41ff-b8b5-a8a5611286ff",
      "metadata": {
        "id": "4050c112-3ff1-41ff-b8b5-a8a5611286ff"
      },
      "source": [
        "Output:\n",
        "The code will generate a contour plot of the Rosenbrock function with the paths taken by the Adagrad,\n",
        "RMSProp, and Adam optimization algorithms, starting from the origin (0, 0).\n",
        "\n",
        "The plot will show how each optimizer converges towards the minimum of the function.\n",
        "\n",
        "Done by:\n",
        "\n",
        "K Karthik Raj(BCD41)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cmKnj3Z8cvH9"
      },
      "id": "cmKnj3Z8cvH9",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}